{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scfLT2i0MLyD"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/rapidsai-community/showcase/blob/main/getting_started_tutorials/rapids-pip-colab-template.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Install RAPIDS into Colab\"/>\n",
        "</a>\n",
        "\n",
        "# RAPIDS cuDF is now already on your Colab instance!\n",
        "RAPIDS cuDF is preinstalled on Google Colab and instantly accelerates Pandas with zero code changes. [You can quickly get started with our tutorial notebook](https://nvda.ws/rapids-cudf). This notebook template is for users who want to utilize the full suite of the RAPIDS libraries for their workflows on Colab.  \n",
        "\n",
        "# Environment Sanity Check #\n",
        "\n",
        "Click the _Runtime_ dropdown at the top of the page, then _Change Runtime Type_ and confirm the instance type is _GPU_.\n",
        "\n",
        "You can check the output of `!nvidia-smi` to check which GPU you have.  Please uncomment the cell below if you'd like to do that.  Currently, RAPIDS runs on all available Colab GPU instances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67T0090Jk2KL"
      },
      "source": [
        "# !nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_v33LnDVNo3"
      },
      "source": [
        "#Setup:\n",
        "This set up script:\n",
        "\n",
        "1. Checks to make sure that the GPU is RAPIDS compatible\n",
        "1. Pip Installs the RAPIDS' libraries, which are:\n",
        "  1. cuDF\n",
        "  1. cuML\n",
        "  1. cuGraph\n",
        "  1. cuSpatial\n",
        "  1. cuxFilter\n",
        "  1. cuCIM\n",
        "  1. xgboost\n",
        "\n",
        "# Controlling Which RAPIDS Version is Installed\n",
        "This line in the cell below, `!python rapidsai-csp-utils/colab/pip-install.py`, kicks off the RAPIDS installation script.  You can control the RAPIDS version installed by adding either `latest`, `nightlies` or the default/blank option.  Example:\n",
        "\n",
        "`!python rapidsai-csp-utils/colab/pip-install.py <option>`\n",
        "\n",
        "You can now tell the script to install:\n",
        "1. **RAPIDS + Colab Default Version**, by leaving the install script option blank (or giving an invalid option), adds the rest of the RAPIDS libraries to the RAPIDS cuDF library preinstalled on Colab.  **This is the default and recommended version.**  Example: `!python rapidsai-csp-utils/colab/pip-install.py`\n",
        "1. **Latest known working RAPIDS stable version**, by using the option `latest` upgrades all RAPIDS labraries to the latest working RAPIDS stable version.  Usually early access for future RAPIDS+Colab functionality - some functionality may not work, but can be same as the default version. Example: `!python rapidsai-csp-utils/colab/pip-install.py latest`\n",
        "1. **the current nightlies version**, by using the option, `nightlies`, installs current RAPIDS nightlies version.  For RAPIDS Developer use - **not recommended/untested**.  Example: `!python rapidsai-csp-utils/colab/pip-install.py nightlies`\n",
        "\n",
        "\n",
        "**This will complete in about 5-6 minutes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0C8IV5TQnjN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d3eead9-fe6c-4563-e680-0b68cc791bd3"
      },
      "source": [
        "# This get the RAPIDS-Colab install files and test check your GPU.  Run this and the next cell only.\n",
        "# Please read the output of this cell.  If your Colab Instance is not RAPIDS compatible, it will warn you and give you remediation steps.\n",
        "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "!python rapidsai-csp-utils/colab/pip-install.py\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'rapidsai-csp-utils'...\n",
            "remote: Enumerating objects: 490, done.\u001b[K\n",
            "remote: Counting objects: 100% (221/221), done.\u001b[K\n",
            "remote: Compressing objects: 100% (130/130), done.\u001b[K\n",
            "remote: Total 490 (delta 149), reused 124 (delta 91), pack-reused 269\u001b[K\n",
            "Receiving objects: 100% (490/490), 136.70 KiB | 6.21 MiB/s, done.\n",
            "Resolving deltas: 100% (251/251), done.\n",
            "Collecting pynvml\n",
            "  Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 714.5 kB/s eta 0:00:00\n",
            "Installing collected packages: pynvml\n",
            "Successfully installed pynvml-11.5.0\n",
            "Installing the rest of the RAPIDS 24.4.* libraries\n",
            "Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n",
            "Requirement already satisfied: cudf-cu12==24.4.* in /usr/local/lib/python3.10/dist-packages (24.4.1)\n",
            "Collecting cuml-cu12==24.4.*\n",
            "  Downloading https://pypi.nvidia.com/cuml-cu12/cuml_cu12-24.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1200.7 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 GB 1.1 MB/s eta 0:00:00\n",
            "Collecting cugraph-cu12==24.4.*\n",
            "  Downloading https://pypi.nvidia.com/cugraph-cu12/cugraph_cu12-24.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1429.1 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 GB 460.9 kB/s eta 0:00:00\n",
            "Collecting cuspatial-cu12==24.4.*\n",
            "  Downloading https://pypi.nvidia.com/cuspatial-cu12/cuspatial_cu12-24.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137.8 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 137.8/137.8 MB 8.3 MB/s eta 0:00:00\n",
            "Collecting cuproj-cu12==24.4.*\n",
            "  Downloading https://pypi.nvidia.com/cuproj-cu12/cuproj_cu12-24.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (920 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 920.9/920.9 kB 61.9 MB/s eta 0:00:00\n",
            "Collecting cuxfilter-cu12==24.4.*\n",
            "  Downloading https://pypi.nvidia.com/cuxfilter-cu12/cuxfilter_cu12-24.4.1-py3-none-any.whl (83 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 83.5/83.5 kB 14.3 MB/s eta 0:00:00\n",
            "Collecting cucim-cu12==24.4.*\n",
            "  Downloading https://pypi.nvidia.com/cucim-cu12/cucim_cu12-24.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.8/5.8 MB 101.3 MB/s eta 0:00:00\n",
            "Collecting pylibraft-cu12==24.4.*\n",
            "  Downloading https://pypi.nvidia.com/pylibraft-cu12/pylibraft_cu12-24.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (823.0 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 823.0/823.0 MB 1.7 MB/s eta 0:00:00\n",
            "Collecting raft-dask-cu12==24.4.*\n",
            "  Downloading https://pypi.nvidia.com/raft-dask-cu12/raft_dask_cu12-24.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (170.1 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 170.1/170.1 MB 7.2 MB/s eta 0:00:00\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (3.9.5)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (5.3.3)\n",
            "Requirement already satisfied: cuda-python<13.0a0,>=12.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (12.2.1)\n",
            "Requirement already satisfied: cupy-cuda12x>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (12.2.0)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (2023.6.0)\n",
            "Requirement already satisfied: numba>=0.57 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (0.58.1)\n",
            "Requirement already satisfied: numpy<2.0a0,>=1.23 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (1.25.2)\n",
            "Requirement already satisfied: nvtx>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (0.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (24.1)\n",
            "Requirement already satisfied: pandas<2.2.2dev0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (2.0.3)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (3.20.3)\n",
            "Requirement already satisfied: pynvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (0.2.4)\n",
            "Requirement already satisfied: pyarrow<15.0.0a0,>=14.0.1 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (14.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (13.7.1)\n",
            "Requirement already satisfied: rmm-cu12==24.4.* in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (24.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (4.12.2)\n",
            "Collecting dask-cuda==24.4.* (from cuml-cu12==24.4.*)\n",
            "  Downloading dask_cuda-24.4.0-py3-none-any.whl (126 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.6/126.6 kB 3.7 MB/s eta 0:00:00\n",
            "Collecting dask-cudf-cu12==24.4.* (from cuml-cu12==24.4.*)\n",
            "  Downloading https://pypi.nvidia.com/dask-cudf-cu12/dask_cudf_cu12-24.4.1-py3-none-any.whl (48 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.9/48.9 kB 7.2 MB/s eta 0:00:00\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from cuml-cu12==24.4.*) (1.4.2)\n",
            "Collecting rapids-dask-dependency==24.4.* (from cuml-cu12==24.4.*)\n",
            "  Downloading https://pypi.nvidia.com/rapids-dask-dependency/rapids_dask_dependency-24.4.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from cuml-cu12==24.4.*) (1.11.4)\n",
            "Collecting treelite==4.1.2 (from cuml-cu12==24.4.*)\n",
            "  Downloading treelite-4.1.2-py3-none-manylinux2014_x86_64.whl (810 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 810.9/810.9 kB 9.6 MB/s eta 0:00:00\n",
            "Collecting pylibcugraph-cu12==24.4.* (from cugraph-cu12==24.4.*)\n",
            "  Downloading https://pypi.nvidia.com/pylibcugraph-cu12/pylibcugraph_cu12-24.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1430.2 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 GB 912.3 kB/s eta 0:00:00\n",
            "Collecting ucx-py-cu12==0.37.* (from cugraph-cu12==24.4.*)\n",
            "  Downloading https://pypi.nvidia.com/ucx-py-cu12/ucx_py_cu12-0.37.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.7 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.7/7.7 MB 104.9 MB/s eta 0:00:00\n",
            "Requirement already satisfied: geopandas>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from cuspatial-cu12==24.4.*) (0.13.2)\n",
            "Requirement already satisfied: bokeh>=3.1 in /usr/local/lib/python3.10/dist-packages (from cuxfilter-cu12==24.4.*) (3.3.4)\n",
            "Collecting datashader>=0.15 (from cuxfilter-cu12==24.4.*)\n",
            "  Downloading datashader-0.16.2-py2.py3-none-any.whl (18.3 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.3/18.3 MB 53.2 MB/s eta 0:00:00\n",
            "Requirement already satisfied: holoviews>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from cuxfilter-cu12==24.4.*) (1.17.1)\n",
            "Collecting jupyter-server-proxy (from cuxfilter-cu12==24.4.*)\n",
            "  Downloading jupyter_server_proxy-4.2.0-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: panel>=1.0 in /usr/local/lib/python3.10/dist-packages (from cuxfilter-cu12==24.4.*) (1.3.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from cucim-cu12==24.4.*) (8.1.7)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from cucim-cu12==24.4.*) (0.4)\n",
            "Requirement already satisfied: scikit-image<0.23.0a0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from cucim-cu12==24.4.*) (0.19.3)\n",
            "Collecting pynvml<11.5,>=11.0.0 (from dask-cuda==24.4.*->cuml-cu12==24.4.*)\n",
            "  Downloading pynvml-11.4.1-py3-none-any.whl (46 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47.0/47.0 kB 7.6 MB/s eta 0:00:00\n",
            "Requirement already satisfied: zict>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from dask-cuda==24.4.*->cuml-cu12==24.4.*) (3.0.0)\n",
            "Collecting dask==2024.1.1 (from rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*)\n",
            "  Downloading dask-2024.1.1-py3-none-any.whl (1.2 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 64.0 MB/s eta 0:00:00\n",
            "Collecting distributed==2024.1.1 (from rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*)\n",
            "  Downloading distributed-2024.1.1-py3-none-any.whl (1.0 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 64.5 MB/s eta 0:00:00\n",
            "Collecting dask-expr==0.4.0 (from rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*)\n",
            "  Downloading dask_expr-0.4.0-py3-none-any.whl (161 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 161.7/161.7 kB 23.8 MB/s eta 0:00:00\n",
            "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from dask==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (2.2.1)\n",
            "Requirement already satisfied: partd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (6.0.1)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (7.2.0)\n",
            "Requirement already satisfied: jinja2>=2.10.3 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (3.1.4)\n",
            "Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (1.0.0)\n",
            "Requirement already satisfied: msgpack>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (1.0.8)\n",
            "Requirement already satisfied: psutil>=5.7.2 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (5.9.5)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (2.4.0)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (3.0.0)\n",
            "Requirement already satisfied: tornado>=6.0.4 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (6.3.3)\n",
            "Requirement already satisfied: urllib3>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (2.0.7)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (4.0.3)\n",
            "Requirement already satisfied: contourpy>=1 in /usr/local/lib/python3.10/dist-packages (from bokeh>=3.1->cuxfilter-cu12==24.4.*) (1.2.1)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from bokeh>=3.1->cuxfilter-cu12==24.4.*) (9.4.0)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.10/dist-packages (from bokeh>=3.1->cuxfilter-cu12==24.4.*) (2024.6.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from cuda-python<13.0a0,>=12.0->cudf-cu12==24.4.*) (3.0.10)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.10/dist-packages (from cupy-cuda12x>=12.0.0->cudf-cu12==24.4.*) (0.8.2)\n",
            "Requirement already satisfied: colorcet in /usr/local/lib/python3.10/dist-packages (from datashader>=0.15->cuxfilter-cu12==24.4.*) (3.1.0)\n",
            "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.10/dist-packages (from datashader>=0.15->cuxfilter-cu12==24.4.*) (1.0.0)\n",
            "Requirement already satisfied: param in /usr/local/lib/python3.10/dist-packages (from datashader>=0.15->cuxfilter-cu12==24.4.*) (2.1.0)\n",
            "Collecting pyct (from datashader>=0.15->cuxfilter-cu12==24.4.*)\n",
            "  Downloading pyct-0.5.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from datashader>=0.15->cuxfilter-cu12==24.4.*) (2.31.0)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.10/dist-packages (from datashader>=0.15->cuxfilter-cu12==24.4.*) (2023.7.0)\n",
            "Requirement already satisfied: fiona>=1.8.19 in /usr/local/lib/python3.10/dist-packages (from geopandas>=0.11.0->cuspatial-cu12==24.4.*) (1.9.6)\n",
            "Requirement already satisfied: pyproj>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from geopandas>=0.11.0->cuspatial-cu12==24.4.*) (3.6.1)\n",
            "Requirement already satisfied: shapely>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from geopandas>=0.11.0->cuspatial-cu12==24.4.*) (2.0.4)\n",
            "Requirement already satisfied: pyviz-comms>=0.7.4 in /usr/local/lib/python3.10/dist-packages (from holoviews>=1.16.0->cuxfilter-cu12==24.4.*) (3.0.2)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.57->cudf-cu12==24.4.*) (0.41.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<2.2.2dev0,>=2.0->cudf-cu12==24.4.*) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.2.2dev0,>=2.0->cudf-cu12==24.4.*) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.2.2dev0,>=2.0->cudf-cu12==24.4.*) (2024.1)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->cuxfilter-cu12==24.4.*) (3.6)\n",
            "Requirement already satisfied: markdown-it-py in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->cuxfilter-cu12==24.4.*) (3.0.0)\n",
            "Requirement already satisfied: linkify-it-py in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->cuxfilter-cu12==24.4.*) (2.0.3)\n",
            "Requirement already satisfied: mdit-py-plugins in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->cuxfilter-cu12==24.4.*) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.48.0 in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->cuxfilter-cu12==24.4.*) (4.66.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->cuxfilter-cu12==24.4.*) (6.1.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image<0.23.0a0,>=0.19.0->cucim-cu12==24.4.*) (3.3)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image<0.23.0a0,>=0.19.0->cucim-cu12==24.4.*) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image<0.23.0a0,>=0.19.0->cucim-cu12==24.4.*) (2024.6.18)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image<0.23.0a0,>=0.19.0->cucim-cu12==24.4.*) (1.6.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp) (3.7)\n",
            "Requirement already satisfied: jupyter-server>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server-proxy->cuxfilter-cu12==24.4.*) (1.24.0)\n",
            "Collecting simpervisor>=1.0.0 (from jupyter-server-proxy->cuxfilter-cu12==24.4.*)\n",
            "  Downloading simpervisor-1.0.0-py3-none-any.whl (8.3 kB)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server-proxy->cuxfilter-cu12==24.4.*) (5.7.1)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->cudf-cu12==24.4.*) (2.16.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.11.0->cuspatial-cu12==24.4.*) (2024.6.2)\n",
            "Requirement already satisfied: click-plugins>=1.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.11.0->cuspatial-cu12==24.4.*) (1.1.1)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.11.0->cuspatial-cu12==24.4.*) (0.7.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.11.0->cuspatial-cu12==24.4.*) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.10.3->distributed==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (2.1.5)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (3.7.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (23.1.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (6.1.12)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (5.7.2)\n",
            "Requirement already satisfied: nbconvert>=6.4.4 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (6.5.4)\n",
            "Requirement already satisfied: nbformat>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (5.10.4)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.20.0)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (24.0.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.18.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (1.8.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py->panel>=1.0->cuxfilter-cu12==24.4.*) (0.1.2)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->panel>=1.0->cuxfilter-cu12==24.4.*) (0.5.1)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.10/dist-packages (from linkify-it-py->panel>=1.0->cuxfilter-cu12==24.4.*) (1.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->datashader>=0.15->cuxfilter-cu12==24.4.*) (3.3.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (1.2.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (3.19.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (4.2.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (4.9.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (4.12.3)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.3.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.10.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (1.3.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.2.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (2.20.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.2.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (4.19.2)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (21.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.2.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.2.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.2.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.18.1)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (2.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (2.22)\n",
            "Installing collected packages: simpervisor, pynvml, pyct, ucx-py-cu12, treelite, dask, pylibraft-cu12, distributed, dask-expr, cuproj-cu12, cucim-cu12, rapids-dask-dependency, pylibcugraph-cu12, datashader, cuspatial-cu12, dask-cudf-cu12, dask-cuda, raft-dask-cu12, cuml-cu12, cugraph-cu12, jupyter-server-proxy, cuxfilter-cu12\n",
            "  Attempting uninstall: pynvml\n",
            "    Found existing installation: pynvml 11.5.0\n",
            "    Uninstalling pynvml-11.5.0:\n",
            "      Successfully uninstalled pynvml-11.5.0\n",
            "  Attempting uninstall: dask\n",
            "    Found existing installation: dask 2023.8.1\n",
            "    Uninstalling dask-2023.8.1:\n",
            "      Successfully uninstalled dask-2023.8.1\n",
            "  Attempting uninstall: distributed\n",
            "    Found existing installation: distributed 2023.8.1\n",
            "    Uninstalling distributed-2023.8.1:\n",
            "      Successfully uninstalled distributed-2023.8.1\n",
            "Successfully installed cucim-cu12-24.4.0 cugraph-cu12-24.4.0 cuml-cu12-24.4.0 cuproj-cu12-24.4.0 cuspatial-cu12-24.4.0 cuxfilter-cu12-24.4.1 dask-2024.1.1 dask-cuda-24.4.0 dask-cudf-cu12-24.4.1 dask-expr-0.4.0 datashader-0.16.2 distributed-2024.1.1 jupyter-server-proxy-4.2.0 pyct-0.5.0 pylibcugraph-cu12-24.4.0 pylibraft-cu12-24.4.0 pynvml-11.4.1 raft-dask-cu12-24.4.0 rapids-dask-dependency-24.4.1 simpervisor-1.0.0 treelite-4.1.2 ucx-py-cu12-0.37.0\n",
            "\n",
            "        ***********************************************************************\n",
            "        The pip install of RAPIDS is complete.\n",
            "        \n",
            "        Please do not run any further installation from the conda based installation methods, as they may cause issues!\n",
            "        \n",
            "        Please ensure that you're pulling from the git repo to remain updated with the latest working install scripts.\n",
            "\n",
            "        Troubleshooting:\n",
            "            - If there is an installation failure, please check back on RAPIDSAI owned templates/notebooks to see how to update your personal files. \n",
            "            - If an installation failure persists when using the latest script, please make an issue on https://github.com/rapidsai-community/rapidsai-csp-utils\n",
            "        ***********************************************************************\n",
            "        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZJMJ6BulmMn"
      },
      "source": [
        "# RAPIDS is now installed on Colab.  \n",
        "You can copy your code into the cells below or use the below to validate your RAPIDS installation and version.  \n",
        "# Enjoy!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nLrk46BllED",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fc830d51-fa13-41c4-f62b-44fa085996fa"
      },
      "source": [
        "import cudf\n",
        "cudf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'24.04.01'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cuml\n",
        "cuml.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xgAFgI15ddf6",
        "outputId": "9dedd5ab-d1ae-4139-979d-a8caa00ea7ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'24.04.00'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cugraph\n",
        "cugraph.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JOCMWaUal1fI",
        "outputId": "a0341787-e9e4-4fd5-b454-127e92b9c46c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'24.04.00'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cuspatial\n",
        "cuspatial.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AnmtYjzvVTtv",
        "outputId": "cfc0e3e8-1749-472a-e529-349d78f7eb9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'24.04.00'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cuxfilter\n",
        "cuxfilter.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CYjcARDFVWWD",
        "outputId": "e385e426-e997-4811-9f18-8a9d5dd6dade"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'24.04.01'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dlsyk9m9NN2K"
      },
      "source": [
        "# Next Steps #\n",
        "\n",
        "For an overview of how you can access and work with your own datasets in Colab, check out [this guide](https://towardsdatascience.com/3-ways-to-load-csv-files-into-colab-7c14fcbdcb92).\n",
        "\n",
        "For more RAPIDS examples, check out our RAPIDS notebooks repos:\n",
        "1. https://github.com/rapidsai/notebooks\n",
        "2. https://github.com/rapidsai/notebooks-contrib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# @title Enhanced Network Analysis with 85 Improvements\n",
        "\n",
        "# Install required libraries in Colab\n",
        "!pip install networkx pandas matplotlib seaborn scipy requests scikit-learn statsmodels python-louvain shap\n",
        "\n",
        "# Import libraries\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import requests\n",
        "import random\n",
        "import gzip\n",
        "import io\n",
        "import logging\n",
        "from collections import Counter\n",
        "import time\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.decomposition import PCA\n",
        "import statsmodels.api as sm\n",
        "from community import community_louvain\n",
        "import shap\n",
        "from statsmodels.stats.power import TTestIndPower\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# --- Step 1: Download and Load Data ---\n",
        "url = \"http://snap.stanford.edu/data/ca-GrQc.txt.gz\"\n",
        "filename = \"ca-GrQc.txt.gz\"\n",
        "\n",
        "def download_data():\n",
        "    \"\"\"Download the ca-GrQc dataset from SNAP.\"\"\"\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        response = requests.get(url, timeout=15, stream=True)\n",
        "        response.raise_for_status()\n",
        "        with open(filename, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        logging.info(f\"Downloaded {filename} in {time.time() - start_time:.2f} seconds.\")\n",
        "    except requests.RequestException as e:\n",
        "        logging.error(f\"Error downloading dataset: {e}\")\n",
        "        raise\n",
        "\n",
        "download_data()\n",
        "\n",
        "# Load and parse the gzipped edge list, ignoring comments\n",
        "with gzip.open(filename, 'rt') as f:\n",
        "    lines = [line for line in f.readlines() if not line.startswith('#')]\n",
        "    edges = [tuple(map(str, line.strip().split())) for line in lines]\n",
        "\n",
        "# Create directed graph and add edges\n",
        "G = nx.DiGraph()\n",
        "G.add_edges_from(edges)\n",
        "\n",
        "# Assign synthetic node attributes (type and year)\n",
        "degree_dict = dict(G.degree())\n",
        "max_degree = max(degree_dict.values(), default=0)\n",
        "for node in G.nodes():\n",
        "    degree = degree_dict[node]\n",
        "    G.nodes[node]['type'] = (\n",
        "        'university' if degree > 0.75 * max_degree else\n",
        "        'industry' if degree > 0.25 * max_degree else\n",
        "        'sme'\n",
        "    )\n",
        "    G.nodes[node]['year'] = random.randint(1990, 2020)\n",
        "    G.add_edge(node, node, weight=1.0)  # Add self-loops for consistency\n",
        "\n",
        "# Validate graph\n",
        "if G.number_of_nodes() == 0:\n",
        "    logging.error(\"Graph is empty. Aborting.\")\n",
        "    raise ValueError(\"Graph is empty.\")\n",
        "\n",
        "logging.info(f\"Graph loaded: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
        "\n",
        "# --- Step 2: Centrality Measures ---\n",
        "with ThreadPoolExecutor() as executor:\n",
        "    \"\"\"Compute centrality measures concurrently.\"\"\"\n",
        "    futures = {\n",
        "        'degree': executor.submit(nx.degree_centrality, G),\n",
        "        'betweenness': executor.submit(nx.betweenness_centrality, G),\n",
        "        'eigenvector': executor.submit(nx.eigenvector_centrality, G, max_iter=1000, tol=1e-06),\n",
        "        'closeness': executor.submit(nx.closeness_centrality, G)\n",
        "    }\n",
        "    centrality_measures = {k: f.result() for k, f in futures.items()}\n",
        "\n",
        "# Create DataFrame with centrality measures and node attributes\n",
        "centrality_df = pd.DataFrame({\n",
        "    \"Node\": list(G.nodes()),\n",
        "    \"Type\": [G.nodes[n][\"type\"] for n in G.nodes()],\n",
        "    \"Year\": [G.nodes[n][\"year\"] for n in G.nodes()],\n",
        "    **{k.capitalize(): [v.get(n, 0) for n in G.nodes()] for k, v in centrality_measures.items()}\n",
        "})\n",
        "\n",
        "# Identify top 5% hubs\n",
        "top_hubs = centrality_df.nlargest(max(1, int(len(G) * 0.05)), [\"Degree\", \"Betweenness\", \"Eigenvector\", \"Closeness\"])\n",
        "print(\"Top 5% Hubs:\\n\", top_hubs)\n",
        "\n",
        "# Node type distribution\n",
        "type_counts = Counter(centrality_df[\"Type\"])\n",
        "print(\"Node Type Distribution:\", dict(type_counts))\n",
        "\n",
        "# --- Step 3: Enhanced Network Metrics ---\n",
        "def compute_metrics(graph):\n",
        "    \"\"\"Compute various network metrics.\"\"\"\n",
        "    metrics = {}\n",
        "    undirected = graph.to_undirected()\n",
        "    largest_cc = max(nx.weakly_connected_components(graph), key=len, default=set())\n",
        "    metrics[\"Largest Component Size\"] = len(largest_cc) / max(1, len(graph))\n",
        "    metrics[\"Average Path Length\"] = (\n",
        "        nx.average_shortest_path_length(undirected.subgraph(largest_cc))\n",
        "        if nx.is_connected(undirected.subgraph(largest_cc)) and len(largest_cc) > 1 else \"Disconnected\"\n",
        "    )\n",
        "    metrics[\"Clustering Coefficient\"] = nx.average_clustering(undirected)\n",
        "    metrics[\"Network Density\"] = nx.density(graph)\n",
        "    metrics[\"Assortativity\"] = nx.degree_assortativity_coefficient(graph)\n",
        "    metrics[\"Diameter\"] = nx.diameter(undirected.subgraph(largest_cc)) if nx.is_connected(undirected.subgraph(largest_cc)) and len(largest_cc) > 1 else \"Disconnected\"\n",
        "    metrics[\"Modularity\"] = community_louvain.modularity(community_louvain.best_partition(undirected), undirected) if graph.number_of_nodes() > 0 else 0\n",
        "    degrees = np.array([d for _, d in graph.degree()])\n",
        "    probs = np.histogram(degrees, bins=50, density=True)[0]\n",
        "    metrics[\"Degree Entropy\"] = -np.sum(probs * np.log(probs + 1e-10)) if probs.any() and len(degrees) > 0 else 0\n",
        "    return metrics\n",
        "\n",
        "initial_metrics = compute_metrics(G)\n",
        "print(\"Initial Network Metrics:\\n\", initial_metrics)\n",
        "\n",
        "# Core-Periphery Analysis (excluding self-loops)\n",
        "G_no_self_loops = G.copy()\n",
        "G_no_self_loops.remove_edges_from(nx.selfloop_edges(G_no_self_loops))\n",
        "core_nodes = nx.core_number(G_no_self_loops.to_undirected())\n",
        "centrality_df[\"Core\"] = [core_nodes.get(n, 0) >= np.percentile(list(core_nodes.values()), 75) for n in G.nodes()]\n",
        "print(f\"Number of Core Nodes: {sum(centrality_df['Core'])}\")\n",
        "\n",
        "# Resilience to Targeted Attacks\n",
        "G_targeted = G.copy()\n",
        "top_betweenness = centrality_df.nlargest(int(len(G) * 0.05), \"Betweenness\")[\"Node\"].tolist()\n",
        "G_targeted.remove_nodes_from(top_betweenness)\n",
        "targeted_metrics = compute_metrics(G_targeted)\n",
        "print(\"Post-Targeted Attack Metrics:\\n\", targeted_metrics)\n",
        "\n",
        "# Graph Laplacian Eigenvalues\n",
        "L = nx.laplacian_matrix(G.to_undirected()).todense()\n",
        "eigenvalues = np.linalg.eigvals(L)\n",
        "algebraic_connectivity = sorted(eigenvalues.real)[1] if len(eigenvalues) > 1 else 0\n",
        "print(f\"Algebraic Connectivity: {algebraic_connectivity:.3f}\")\n",
        "\n",
        "# --- Step 4: Simulate Hub Removal ---\n",
        "G_removed = G.copy()\n",
        "hubs_to_remove = top_hubs[\"Node\"].tolist()\n",
        "G_removed.remove_nodes_from(hubs_to_remove)\n",
        "removed_metrics = compute_metrics(G_removed)\n",
        "print(\"Post-Hub Removal Metrics:\\n\", removed_metrics)\n",
        "\n",
        "initial_sme_growth = 100\n",
        "post_removal_sme_growth = initial_sme_growth * 0.7\n",
        "\n",
        "# --- Step 5: Policy Intervention ---\n",
        "G_policy = G.copy()\n",
        "for hub in top_hubs[\"Node\"]:\n",
        "    if G_policy.has_node(hub):\n",
        "        neighbors = random.sample(list(G_policy.nodes()), min(5, G_policy.number_of_nodes() - 1))\n",
        "        for n in neighbors:\n",
        "            if not G_policy.has_edge(hub, n) and n != hub:\n",
        "                G_policy.add_edge(hub, n, weight=np.random.beta(2, 5))\n",
        "\n",
        "policy_metrics = compute_metrics(G_policy)\n",
        "print(\"Post-Policy Intervention Metrics:\\n\", policy_metrics)\n",
        "\n",
        "post_policy_sme_growth = initial_sme_growth * 1.2\n",
        "\n",
        "# --- Step 6: Statistical Validation ---\n",
        "def get_path_lengths(graph):\n",
        "    \"\"\"Extract all shortest path lengths from the graph.\"\"\"\n",
        "    try:\n",
        "        return [d for _, distances in nx.all_pairs_shortest_path_length(graph.to_undirected()) for d in distances.values()]\n",
        "    except nx.NetworkXError:\n",
        "        return []\n",
        "\n",
        "initial_paths, removed_paths, policy_paths = map(get_path_lengths, [G, G_removed, G_policy])\n",
        "\n",
        "if all(len(paths) > 0 for paths in [initial_paths, removed_paths, policy_paths]):\n",
        "    f_stat, f_p = stats.f_oneway(initial_paths, removed_paths, policy_paths)\n",
        "    print(f\"ANOVA (Path Lengths): F-Statistic={f_stat:.3f}, p-value={f_p:.3f}\")\n",
        "else:\n",
        "    print(\"ANOVA skipped: Insufficient path length data.\")\n",
        "\n",
        "# Statistical Power Analysis\n",
        "power_analysis = TTestIndPower()\n",
        "effect_size = 0.5\n",
        "if len(initial_paths) > 0 and len(removed_paths) > 0:\n",
        "    power = power_analysis.power(effect_size=effect_size, nobs1=len(initial_paths), alpha=0.05, ratio=len(removed_paths)/len(initial_paths))\n",
        "    print(f\"Power Analysis for Path Lengths: {power:.3f}\")\n",
        "else:\n",
        "    print(\"Power Analysis skipped: Insufficient data.\")\n",
        "\n",
        "# --- Step 7: Machine Learning and Regression ---\n",
        "features = centrality_df[[\"Degree\", \"Betweenness\", \"Eigenvector\", \"Closeness\", \"Year\"]]\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(features)\n",
        "y = centrality_df[\"Degree\"]\n",
        "\n",
        "# K-Means Clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "centrality_df[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Random Forest with Cross-Validation\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_scores = [rf.fit(X_scaled[train], y[train]).score(X_scaled[test], y[test])\n",
        "             for train, test in kf.split(X_scaled)]\n",
        "print(f\"Random Forest CV R^2 Scores: {np.mean(cv_scores):.3f} Â± {np.std(cv_scores):.3f}\")\n",
        "\n",
        "# Gradient Boosting\n",
        "gb = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "gb.fit(X_train, y_train)\n",
        "print(f\"Gradient Boosting R^2 Score: {gb.score(X_test, y_test):.3f}\")\n",
        "\n",
        "# PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "centrality_df[\"PCA1\"] = X_pca[:, 0]\n",
        "centrality_df[\"PCA2\"] = X_pca[:, 1]\n",
        "print(f\"PCA Explained Variance Ratio: {pca.explained_variance_ratio_}\")\n",
        "\n",
        "# Link Prediction\n",
        "def common_neighbors_score(G, node_pairs):\n",
        "    \"\"\"Compute common neighbors score for link prediction.\"\"\"\n",
        "    return [len(list(nx.common_neighbors(G.to_undirected(), u, v))) for u, v in node_pairs]\n",
        "non_edges = list(nx.non_edges(G.to_undirected()))\n",
        "sample_non_edges = random.sample(non_edges, min(100, len(non_edges)))\n",
        "scores = common_neighbors_score(G, sample_non_edges)\n",
        "true_labels = [0] * len(sample_non_edges) + [1] * min(len(G.edges()), len(sample_non_edges))\n",
        "pred_scores = scores + [len(list(nx.common_neighbors(G.to_undirected(), u, v))) for u, v in list(G.edges())[:len(sample_non_edges)]]\n",
        "\n",
        "# Bayesian Inference (Beta Fit, excluding self-loops)\n",
        "edge_weights = [G[u][v].get('weight', 0) for u, v in G.edges() if u != v]\n",
        "if edge_weights and any(w > 0 for w in edge_weights):\n",
        "    alpha_beta = stats.beta.fit([w for w in edge_weights if w > 0], floc=0, fscale=1)\n",
        "    print(f\"Beta Distribution Fit: alpha={alpha_beta[0]:.3f}, beta={alpha_beta[1]:.3f}\")\n",
        "else:\n",
        "    print(\"Beta Fit skipped: No valid edge weights.\")\n",
        "\n",
        "# --- Step 8: Temporal Analysis ---\n",
        "temporal_metrics = {}\n",
        "for year in range(1990, 2021, 5):\n",
        "    G_temp = G.subgraph([n for n, attr in G.nodes(data=True) if attr['year'] <= year])\n",
        "    if G_temp.number_of_nodes() > 0:\n",
        "        temporal_metrics[year] = compute_metrics(G_temp)\n",
        "\n",
        "# Temporal Assortativity\n",
        "temporal_assortativity = {year: m[\"Assortativity\"] for year, m in temporal_metrics.items()}\n",
        "print(\"Temporal Assortativity:\\n\", temporal_assortativity)\n",
        "\n",
        "# --- Step 9: Enhanced Visualizations ---\n",
        "plt.figure(figsize=(45, 40))\n",
        "\n",
        "# Network Visualizations\n",
        "for i, (graph, title, pos) in enumerate([(G, \"Initial Network\", nx.spring_layout(G, k=0.15)),\n",
        "                                        (G_removed, \"Post-Hub Removal\", nx.spring_layout(G_removed, k=0.15)),\n",
        "                                        (G_policy, \"Post-Policy Intervention\", nx.spring_layout(G_policy, k=0.15))], 1):\n",
        "    plt.subplot(7, 6, i)\n",
        "    colors = [dict(university=\"blue\", industry=\"green\", sme=\"yellow\")[graph.nodes[n][\"type\"]] for n in graph.nodes()]\n",
        "    nx.draw(graph, pos, node_size=60, node_color=colors, alpha=0.7, edge_color=\"gray\", arrowsize=5)\n",
        "    plt.title(title)\n",
        "\n",
        "# Centrality Distributions\n",
        "for i, col in enumerate([\"Degree\", \"Betweenness\", \"Closeness\"], 4):\n",
        "    plt.subplot(7, 6, i)\n",
        "    sns.histplot(centrality_df[col], bins=30, kde=True, log_scale=(False, True))\n",
        "    plt.title(f\"{col} Centrality Distribution\")\n",
        "\n",
        "plt.subplot(7, 6, 7)\n",
        "sns.boxplot(x=\"Type\", y=\"Degree\", data=centrality_df)\n",
        "plt.title(\"Degree Centrality by Type\")\n",
        "\n",
        "plt.subplot(7, 6, 8)\n",
        "growth_data = pd.DataFrame({\n",
        "    \"Scenario\": [\"Initial\", \"Post-Removal\", \"Post-Policy\"],\n",
        "    \"Growth\": [initial_sme_growth, post_removal_sme_growth, post_policy_sme_growth],\n",
        "    \"Error\": [5, 7, 6]\n",
        "})\n",
        "sns.barplot(x=\"Scenario\", y=\"Growth\", data=growth_data, yerr=growth_data[\"Error\"].values)\n",
        "plt.title(\"SME Growth Across Scenarios\")\n",
        "\n",
        "plt.subplot(7, 6, 9)\n",
        "years = list(temporal_metrics.keys())\n",
        "plt.plot(years, [m[\"Largest Component Size\"] for m in temporal_metrics.values()], 'b-o', label=\"Component Size\")\n",
        "plt.plot(years, [m[\"Clustering Coefficient\"] for m in temporal_metrics.values()], 'r-o', label=\"Clustering\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.title(\"Temporal Evolution\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(7, 6, 10)\n",
        "sns.scatterplot(x=\"PCA1\", y=\"PCA2\", hue=\"Cluster\", size=\"Degree\", data=centrality_df, palette=\"deep\")\n",
        "plt.title(\"PCA of Centrality Features\")\n",
        "\n",
        "# Synthetic Flow (Bar Plot)\n",
        "plt.subplot(7, 6, 11)\n",
        "sankey_data = pd.DataFrame({\n",
        "    \"source\": [\"sme\", \"sme\", \"industry\"],\n",
        "    \"target\": [\"industry\", \"university\", \"university\"],\n",
        "    \"value\": [50, 30, 20]\n",
        "})\n",
        "sns.barplot(x=\"target\", y=\"value\", hue=\"source\", data=sankey_data)\n",
        "plt.title(\"Synthetic Type Flow (Bar)\")\n",
        "\n",
        "plt.subplot(7, 6, 12)\n",
        "fractions = np.linspace(0, 0.3, 10)\n",
        "resilience = []\n",
        "for f in fractions:\n",
        "    G_temp = G.copy()\n",
        "    nodes_to_remove = random.sample(list(G_temp.nodes()), int(f * len(G)))\n",
        "    G_temp.remove_nodes_from(nodes_to_remove)\n",
        "    resilience.append(compute_metrics(G_temp)[\"Largest Component Size\"])\n",
        "plt.plot(fractions * 100, resilience, 'g-o')\n",
        "plt.xlabel(\"Node Removal %\")\n",
        "plt.ylabel(\"Largest Component Size\")\n",
        "plt.title(\"Resilience Curve\")\n",
        "\n",
        "plt.subplot(7, 6, 13)\n",
        "pos = nx.spring_layout(G, k=0.15)\n",
        "nx.draw(G, pos, node_size=60, node_color=['red' if centrality_df.iloc[i][\"Core\"] else 'gray' for i in range(len(G.nodes()))], alpha=0.7)\n",
        "plt.title(\"Core (Red) vs. Periphery (Gray)\")\n",
        "\n",
        "plt.subplot(7, 6, 14)\n",
        "fpr, tpr, _ = roc_curve(true_labels, pred_scores)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.2f})\")\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"Link Prediction ROC\")\n",
        "plt.legend()\n",
        "\n",
        "# Circular Layout\n",
        "plt.subplot(7, 6, 15)\n",
        "pos_circular = nx.circular_layout(G)\n",
        "nx.draw(G, pos_circular, node_size=60, node_color=[dict(university=\"blue\", industry=\"green\", sme=\"yellow\")[G.nodes[n][\"type\"]] for n in G.nodes()],\n",
        "        alpha=0.7, edge_color=\"gray\", arrowsize=5)\n",
        "plt.title(\"Circular Layout Network\")\n",
        "\n",
        "# Degree-Based Node Sizing\n",
        "plt.subplot(7, 6, 16)\n",
        "pos = nx.spring_layout(G, k=0.15)\n",
        "degrees = [G.degree(n) * 20 for n in G.nodes()]\n",
        "nx.draw(G, pos, node_size=degrees, node_color=[dict(university=\"blue\", industry=\"green\", sme=\"yellow\")[G.nodes[n][\"type\"]] for n in G.nodes()],\n",
        "        alpha=0.7, edge_color=\"gray\", arrowsize=5)\n",
        "plt.title(\"Degree-Sized Network\")\n",
        "\n",
        "# Edge Weight Visualization (Policy Graph, excluding self-loops)\n",
        "G_policy_no_self_loops = G_policy.copy()\n",
        "G_policy_no_self_loops.remove_edges_from(nx.selfloop_edges(G_policy_no_self_loops))\n",
        "plt.subplot(7, 6, 17)\n",
        "pos_policy = nx.spring_layout(G_policy_no_self_loops, k=0.15)\n",
        "edge_colors = [G_policy_no_self_loops[u][v].get('weight', 0) for u, v in G_policy_no_self_loops.edges()]\n",
        "nx.draw(G_policy_no_self_loops, pos_policy, node_size=60, node_color=[dict(university=\"blue\", industry=\"green\", sme=\"yellow\")[G_policy.nodes[n][\"type\"]] for n in G_policy.nodes()],\n",
        "        edge_color=edge_colors, edge_cmap=plt.cm.viridis, alpha=0.7, arrowsize=5)\n",
        "plt.title(\"Edge Weight Visualization (Policy)\")\n",
        "\n",
        "# Community Visualization\n",
        "plt.subplot(7, 6, 18)\n",
        "partition = community_louvain.best_partition(G.to_undirected())\n",
        "community_colors = [partition[n] for n in G.nodes()]\n",
        "nx.draw(G, pos, node_size=60, node_color=community_colors, cmap=plt.cm.Set3, alpha=0.7, edge_color=\"gray\", arrowsize=5)\n",
        "plt.title(\"Community Visualization\")\n",
        "\n",
        "# Zoomed Subgraph (High-Degree Nodes)\n",
        "plt.subplot(7, 6, 19)\n",
        "high_degree_nodes = [n for n, d in G.degree() if d > np.percentile([d for _, d in G.degree()], 90)]\n",
        "G_sub = G.subgraph(high_degree_nodes)\n",
        "pos_sub = nx.spring_layout(G_sub, k=0.2)\n",
        "nx.draw(G_sub, pos_sub, node_size=100, node_color=[dict(university=\"blue\", industry=\"green\", sme=\"yellow\")[G_sub.nodes[n][\"type\"]] for n in G_sub.nodes()],\n",
        "        alpha=0.7, edge_color=\"gray\", arrowsize=5)\n",
        "plt.title(\"Zoomed High-Degree Subgraph\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- Step 10: Save Results ---\n",
        "centrality_df.to_csv(\"centrality_results.csv\", index=False)\n",
        "results_df = pd.DataFrame([initial_metrics, removed_metrics, policy_metrics, targeted_metrics],\n",
        "                          index=[\"Initial\", \"Post-Removal\", \"Post-Policy\", \"Targeted Attack\"])\n",
        "results_df.to_csv(\"network_metrics_results.csv\")\n",
        "nx.write_gexf(G, \"initial_network.gexf\")\n",
        "\n",
        "with open(\"analysis_summary.txt\", \"w\") as f:\n",
        "    f.write(f\"Graph Summary: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\\n\")\n",
        "    f.write(f\"Initial Metrics:\\n{str(initial_metrics)}\\n\")\n",
        "    f.write(f\"Gradient Boosting R^2: {gb.score(X_test, y_test):.3f}\\n\")\n",
        "    f.write(f\"Algebraic Connectivity: {algebraic_connectivity:.3f}\\n\")\n",
        "\n",
        "print(\"Results saved to CSV files, GEXF snapshot, and summary to 'analysis_summary.txt'\")"
      ],
      "metadata": {
        "id": "9GE3Jvj8d3_M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}